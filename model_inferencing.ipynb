{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3510fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from Utils.Data_Preprocessing import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"\n",
    "    Load both .pkl and .h5 models from the Models directory.\n",
    "    Simply loads and prints model types without doing any inference.\n",
    "    \"\"\"\n",
    "    # Define path to your Models directory\n",
    "    project_root = Path('.')  # Current directory\n",
    "    models_dir = project_root / 'Models'\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not models_dir.exists():\n",
    "        print(f\"Error: Models directory not found at {models_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading models from: {models_dir}\")\n",
    "    \n",
    "    # Define model files based on your directory structure\n",
    "    pkl_models = [\n",
    "        'decision_tree_model.pkl',\n",
    "        'linear_regression_model.pkl',\n",
    "        'prophet_model.pkl',\n",
    "        'random_forest_model.pkl',\n",
    "        'slinear_regression_model.pkl',\n",
    "        'svr_model.pkl',\n",
    "        'voting_model.pkl',\n",
    "        'xgboost_model.pkl'\n",
    "    ]\n",
    "    \n",
    "    h5_models = [\n",
    "        'gru_model.h5',\n",
    "        'lstm_model.h5'\n",
    "    ]\n",
    "    \n",
    "    # Dictionary to store loaded models\n",
    "    loaded_models = {}\n",
    "    \n",
    "    # Load pickle models\n",
    "    print(\"\\n--- Loading Pickle Models ---\")\n",
    "    for model_file in pkl_models:\n",
    "        model_path = models_dir / model_file\n",
    "        try:\n",
    "            # Only try to load if file exists\n",
    "            if model_path.exists():\n",
    "                model = joblib.load(model_path)\n",
    "                model_name = model_path.stem  # Get filename without extension\n",
    "                loaded_models[model_name] = model\n",
    "                \n",
    "                # Check model type\n",
    "                model_type = type(model).__name__\n",
    "                has_predict = hasattr(model, 'predict')\n",
    "                \n",
    "                print(f\"✓ {model_name}: Type={model_type}, Has predict method={has_predict}\")\n",
    "            else:\n",
    "                print(f\"⚠ {model_file} not found, skipping\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {model_file}: {e}\")\n",
    "    \n",
    "    # Load h5 models (TensorFlow/Keras models)\n",
    "    print(\"\\n--- Loading H5 Models ---\")\n",
    "    for model_file in h5_models:\n",
    "        model_path = models_dir / model_file\n",
    "        try:\n",
    "            # Only try to load if file exists\n",
    "            if model_path.exists():\n",
    "                # TensorFlow requires string path\n",
    "                model = tf.keras.models.load_model(str(model_path))\n",
    "                model_name = model_path.stem  # Get filename without extension\n",
    "                loaded_models[model_name] = model\n",
    "                \n",
    "                # Check model type and summary\n",
    "                model_type = type(model).__name__\n",
    "                has_predict = hasattr(model, 'predict')\n",
    "                \n",
    "                print(f\"✓ {model_name}: Type={model_type}, Has predict method={has_predict}\")\n",
    "                \n",
    "                # Print model summary\n",
    "                print(f\"Model structure:\")\n",
    "                model.summary()\n",
    "            else:\n",
    "                print(f\"⚠ {model_file} not found, skipping\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {model_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len(loaded_models)} models\")\n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96fbc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from: Models\n",
      "\n",
      "--- Loading Pickle Models ---\n",
      "✓ decision_tree_model: Type=DecisionTreeRegressor, Has predict method=True\n",
      "✓ linear_regression_model: Type=LinearRegression, Has predict method=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ prophet_model: Type=Prophet, Has predict method=True\n",
      "✓ random_forest_model: Type=RandomForestRegressor, Has predict method=True\n",
      "⚠ slinear_regression_model.pkl not found, skipping\n",
      "✓ svr_model: Type=SVR, Has predict method=True\n",
      "✓ voting_model: Type=VotingRegressor, Has predict method=True\n",
      "✓ xgboost_model: Type=XGBRegressor, Has predict method=True\n",
      "\n",
      "--- Loading H5 Models ---\n",
      "✓ gru_model: Type=Sequential, Has predict method=True\n",
      "Model structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">211,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">158,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │        \u001b[38;5;34m70,650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_4 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m200\u001b[0m)         │       \u001b[38;5;34m211,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m200\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_5 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │       \u001b[38;5;34m158,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │        \u001b[38;5;34m14,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m97\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">454,844</span> (1.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m454,844\u001b[0m (1.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">454,843</span> (1.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m454,843\u001b[0m (1.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> (8.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1\u001b[0m (8.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ lstm_model: Type=Sequential, Has predict method=True\n",
      "Model structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">93,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">280,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │        \u001b[38;5;34m93,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │       \u001b[38;5;34m280,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m12,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">387,331</span> (1.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m387,331\u001b[0m (1.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">387,329</span> (1.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m387,329\u001b[0m (1.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 9 models\n"
     ]
    }
   ],
   "source": [
    "loaded_models= load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581866e",
   "metadata": {},
   "source": [
    "## Classical Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32a4913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ decision_tree_model prediction: 23.0167\n",
      "✓ linear_regression_model prediction: 23.1745\n",
      "✗ Error predicting with prophet_model: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "✓ random_forest_model prediction: 22.8487\n",
      "✓ svr_model prediction: 46.4411\n",
      "✓ voting_model prediction: 28.8450\n",
      "✓ xgboost_model prediction: 22.9157\n",
      "✓ decision_tree_model prediction: 331.8833\n",
      "✓ linear_regression_model prediction: 342.9801\n",
      "✗ Error predicting with prophet_model: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "✓ random_forest_model prediction: 339.9581\n",
      "✓ svr_model prediction: 351.0005\n",
      "✓ voting_model prediction: 341.3777\n",
      "✓ xgboost_model prediction: 331.5721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SVR was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decision_tree_model': np.float64(331.8833312988281),\n",
       " 'linear_regression_model': np.float64(342.98010733496636),\n",
       " 'random_forest_model': np.float64(339.95813232421875),\n",
       " 'svr_model': np.float64(351.00048754605604),\n",
       " 'voting_model': np.float64(341.3777125941356),\n",
       " 'xgboost_model': np.float32(331.5721)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference_classical_models(data):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "\n",
    "    # Load the scalers\n",
    "    scaler_X = joblib.load('Scaler/scaler_X_minmax.pkl')\n",
    "    scaler_y = joblib.load('Scaler/scaler_y_minmax.pkl')\n",
    "    #model = joblib.load('Models/linear_regression_model.pkl')\n",
    "    \n",
    "    \n",
    "    # Filter only classical models (skip keras models which are of type 'Functional', 'Sequential', etc.)\n",
    "    classical_models = {\n",
    "        name: model for name, model in loaded_models.items()\n",
    "        if not isinstance(model, tf.keras.Model)\n",
    "    }\n",
    "\n",
    "    # # Read last 20 rows (not 19!) for moving average support\n",
    "    # df_hist = pd.read_csv('Data/Tasla_Stock_Updated_V2.csv').tail(20)\n",
    "\n",
    "    # # Add the new data (must match expected column names)\n",
    "    # df_new = pd.DataFrame([data], columns=['Low', 'High', 'Open', 'Close', 'Volume'])\n",
    "    # df_combined = pd.concat([df_hist, df_new], ignore_index=True)\n",
    "    \n",
    "    # # Select features for prediction (same as used during training)\n",
    "    # features = ['Low', 'High', 'Open', 'Close', 'Volume']\n",
    "    # # Extract the last row (new data) with features\n",
    "    # X = df_combined[features].iloc[-1:].values  # Shape: (1, n_features)\n",
    "\n",
    "    # Scale the features\n",
    "    X_scaled = scaler_X.transform(np.array(data).reshape(1, -1))\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    # Inference using each classical model\n",
    "    for name, model in classical_models.items():\n",
    "        try:\n",
    "            y_pred_scaled = model.predict(X_scaled)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "            predictions[name] = y_pred[0][0]\n",
    "            print(f\"✓ {name} prediction: {y_pred[0][0]:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error predicting with {name}: {e}\")\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "\n",
    "inference_classical_models([23.543333053588867,23.83133316040039,22.858667373657227,22.99933242797852,114088500])\n",
    "inference_classical_models( [347.7366638183594,349.4800109863281,340.8133239746094,341.8299865722656,55013700])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df84271",
   "metadata": {},
   "source": [
    "## LSTM models inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db02279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "✓ gru_model prediction: 448.7924\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "✓ lstm_model prediction: 403.6240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibabdullah/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "def inference_lstm_models_with_history(single_row, window_size=5, tweak_pct=0.01):\n",
    "    \"\"\"\n",
    "    Perform inference for LSTM/GRU models using a sequence made from\n",
    "    a synthetic sequence of the latest (window_size - 1) rows slightly tweaked from\n",
    "    the input row + current input row.\n",
    "\n",
    "    Args:\n",
    "        single_row (list or np.array): Latest input features (1 timestep).\n",
    "        dataset_path (str): Path to the CSV dataset file (only used for scaler loading).\n",
    "        window_size (int): Number of timesteps expected by the model.\n",
    "        tweak_pct (float): Max percentage tweak (+/-) to apply to generate synthetic previous steps.\n",
    "\n",
    "    Returns:\n",
    "        dict: Predictions from each LSTM/GRU model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load scalers\n",
    "    scaler_X = joblib.load('Scaler/scaler_X_minmax.pkl')\n",
    "    scaler_y = joblib.load('Scaler/scaler_y_minmax.pkl')\n",
    "\n",
    "    # Assuming loaded_models is globally defined or imported:\n",
    "    rnn_models = {\n",
    "        name: model for name, model in loaded_models.items()\n",
    "        if isinstance(model, tf.keras.Model)\n",
    "    }\n",
    "\n",
    "    features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "    # Create synthetic sequence by tweaking single_row for previous (window_size - 1) timesteps\n",
    "    synthetic_sequence = []\n",
    "    for i in range(window_size - 1):\n",
    "        # Apply random tweak within ±tweak_pct\n",
    "        tweak_factors = 1 + np.random.uniform(-tweak_pct, tweak_pct, size=len(single_row))\n",
    "        synthetic_step = single_row * tweak_factors\n",
    "        synthetic_sequence.append(synthetic_step)\n",
    "\n",
    "    # Convert list to np.array\n",
    "    synthetic_sequence = np.array(synthetic_sequence)\n",
    "\n",
    "    # Append the original single_row as the last timestep\n",
    "    full_sequence = np.vstack([synthetic_sequence, single_row])\n",
    "\n",
    "    # Scale the sequence\n",
    "    seq_scaled = scaler_X.transform(full_sequence)\n",
    "\n",
    "    # Reshape for model input (1, window_size, features)\n",
    "    seq_scaled = seq_scaled.reshape(1, window_size, len(features))\n",
    "\n",
    "    predictions = {}\n",
    "    for name, model in rnn_models.items():\n",
    "        try:\n",
    "            y_pred_scaled = model.predict(seq_scaled)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "            predictions[name] = y_pred[0][0]\n",
    "            print(f\"✓ {name} prediction: {y_pred[0][0]:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error predicting with {name}: {e}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_row = np.array([475.90,483.99,457.51,479.86,131223000])\n",
    "dataset_path = 'Data/Tasla_Stock_Updated_V2.csv'  # Only used for scaler loading here\n",
    "\n",
    "predictions = inference_lstm_models_with_history(input_row, window_size=5, tweak_pct=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
